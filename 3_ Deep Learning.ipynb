{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision using OpenCV\n",
    "* BGR channels: ranges from 0 to 255\n",
    "* Grayscale: 1 channel, ranges from 0 to 255, 0 = Black, 255 = White\n",
    "* !pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Show, Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'pic1.jpg'\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(img_path,1) # 1 --> Color, 0 --> Grayscale\n",
    "img_shape = img.shape\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Save image\n",
    "# cv2.imwrite('saved image', img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grayscale, Resize, Crop, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Resize image\n",
    "resize_shape = (224,224)\n",
    "\n",
    "resize_img_1 = cv2.resize(img, None,fx=0.5, fy=0.5)\n",
    "cv2.imshow('image',resize_img_1)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "resize_img_2 = cv2.resize(img, resize_shape)\n",
    "cv2.imshow('image',resize_img_2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Crop image with indexing\n",
    "cropped_img = img[10:100,10:100]\n",
    "cv2.imshow('image',cropped_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw a Line, Circle, Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Black image\n",
    "black_img = np.zeros((224,224,3))\n",
    "cv2.imshow('image',black_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create rectangle\n",
    "rect = cv2.rectangle(black_img, (1,1),(100,100),(255,0,0),5)\n",
    "cv2.imshow('image',rect)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create line\n",
    "line = cv2.line(black_img, (1,1),(100,100),(255,0,0),5)\n",
    "cv2.imshow('image',line)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create circle\n",
    "circle = cv2.circle(black_img, (100,100), 10, (255,0,0),5)\n",
    "cv2.imshow('image',circle)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate, Flip, Change perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate image\n",
    "rotate_img_matrix = cv2.getRotationMatrix2D((img.shape[0], img.shape[1]) , 45,0.5)\n",
    "rotate_img = cv2.warpAffine(img, rotate_img_matrix, (img.shape[0], img.shape[1]))\n",
    "cv2.imshow('image',rotate_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Flip image\n",
    "flip_img = cv2.flip(img,1)\n",
    "cv2.imshow('image',flip_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Change perspective\n",
    "height = 100\n",
    "width = 100\n",
    "\n",
    "orig_img_coor = np.float32([[100,100], [200,100], [100,200], [200,200]])\n",
    "new_img_coor = np.float32([[0,0], [width,0], [0,height], [width,height]])\n",
    "perspective_img_matrix = cv2.getPerspectiveTransform(orig_img_coor, new_img_coor)\n",
    "perspective_img = cv2.warpPerspective(img, perspective_img_matrix, (width,height) )\n",
    "cv2.imshow('image',perspective_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness and Darkness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intensity = 50\n",
    "intensity_matrix = np.ones(img.shape, dtype='uint8') * intensity\n",
    "\n",
    "bright_img = cv2.add(img,intensity_matrix)\n",
    "cv2.imshow('image',bright_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "dark_img = cv2.subtract(img,intensity_matrix)\n",
    "cv2.imshow('image',dark_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blur, Sharpening, Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size_5X5 = np.ones((5,5)) / 25\n",
    "kernel_size_5X5_tuple = (5,5)\n",
    "\n",
    "# Gaussian blur\n",
    "blur_gaussian = cv2.GaussianBlur(img, kernel_size_5X5_tuple, 1)\n",
    "cv2.imshow('image',blur_gaussian)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# filter2D blur\n",
    "blur_filter2D = cv2.filter2D(img, -1, kernel_size_5X5)\n",
    "cv2.imshow('image',blur_filter2D)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Sharpening kernel\n",
    "sharp_filter = np.array([[-1,-1,-1],[-1,10,-1],[-1,-1,-1]])\n",
    "# Sharp image\n",
    "sharp_img = cv2.filter2D(img,-1, sharp_filter)\n",
    "cv2.imshow('image', sharp_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Check out cv2.THRESH_BINARY_INV, cv2.THRESH_TRUNC \n",
    "# Values below 100 set to 0 (black), Values above 100 set to 255 (white)\n",
    "ret, threshold_img = cv2.threshold(gray_img, 100, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('image', threshold_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Transformations\n",
    "* Erosion: Background erodes\n",
    "* Dilation: Background dilates\n",
    "* Opening = Erosion >>> Dilation\n",
    "* Closing = Dilation >>> Erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size_5X5 = np.ones((5,5)) / 25\n",
    "\n",
    "# Erosion\n",
    "erosion_img = cv2.erode(img, kernel_size_5X5)\n",
    "cv2.imshow('image', erosion_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Dilation\n",
    "dilation_img = cv2.dilate(img,kernel_size_5X5)\n",
    "cv2.imshow('image', dilation_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Opening operation for noise removal\n",
    "opening_operation = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel_size_5X5)\n",
    "cv2.imshow('image', opening_operation)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Closing operation for noise removal\n",
    "closing_operation = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel_size_5X5)\n",
    "cv2.imshow('image', closing_operation)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection\n",
    "* Lapacian, Canny, Sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canny Edge Detection\n",
    "canny_edge_img = cv2.Canny(img, 10,100)\n",
    "cv2.imshow('image', canny_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Laplacian Edge Detection\n",
    "laplacian_edge_img = cv2.Laplacian(img, cv2.CV_64F)\n",
    "cv2.imshow('image', laplacian_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_detector_object = cv2.SimpleBlobDetector_create()\n",
    "keypoint_info = blob_detector_object.detect(img)\n",
    "blank_img = np.zeros((1,1))\n",
    "blob_detector = cv2.drawKeypoints(img,keypoint_info, np.array([]), \n",
    "                                  (255,0,0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv2.imshow('image', blob_detector)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Detection\n",
    "* Probabilistic Hough Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# detect canny edges\n",
    "canny_edge_img = cv2.Canny(gray_img, 10,100)\n",
    "cv2.imshow('image', canny_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Probabilistic Hough Lines\n",
    "line_info = cv2.HoughLinesP(canny_edge_img, 1, np.pi/180, 250, 110, 10)\n",
    "\n",
    "while line_info is True:\n",
    "    for i in line_info:\n",
    "        for x1,y1,x2,y2 in i:\n",
    "            cv2.line(img, (x1,y1), (x2,y2), (255,0,0), 2)\n",
    "\n",
    "cv2.imshow('image', gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Detection, Approximating contours and Convex Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_copy = img.copy()\n",
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# detect canny edges\n",
    "canny_edge_img = cv2.Canny(gray_img, 10,100)\n",
    "cv2.imshow('image', canny_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Contour detection\n",
    "contours, hierarchy = cv2.findContours(canny_edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "contour_img = cv2.drawContours(img_copy, contours,-1,(255,0,0), 4)\n",
    "cv2.imshow('image', contour_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Bounding rectangle around each contour\n",
    "for i in contours:\n",
    "    x,y,w,h = cv2.boundingRect(i)\n",
    "    bounding_rect_img = cv2.rectangle(img_copy, (x,y),(x+w,y+h), (0,0,255),1)\n",
    "    cv2.imshow('image', bounding_rect_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Contour Approximation through each detected contour\n",
    "for i in contours:\n",
    "    accuracy = 0.04 * cv2.arcLength(i, True)\n",
    "    approx = cv2.approxPolyDP(i, accuracy, True)\n",
    "    approx_contour_img = cv2.drawContours(img_copy, [approx],-1,(255,0,0), 4)\n",
    "    cv2.imshow('image', approx_contour_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Convex Hull:\n",
    "\n",
    "# Sort the contours by area and remove the largest contour\n",
    "n = len(contours) - 1\n",
    "contours_sorted = sorted(contours, key=cv2.contourArea, reverse=False)[:n]\n",
    "\n",
    "# Convex Hull through each detected contour\n",
    "for i in contours:\n",
    "    hull = cv2.convexHull(i)\n",
    "    hull_contour_img = cv2.drawContours(img_copy, [hull], -1, (255,0,0), 2)\n",
    "    cv2.imshow('image', hull_contour_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# # Matching contour shapes: compare input image and target image\n",
    "\n",
    "# # contours, hierarchy = cv2.findContours(input_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "# # In input image, First largest contour is the boundary of the entire image\n",
    "# second_largest_contour = contours[1]\n",
    "\n",
    "# # In target image\n",
    "# # contours, hierarchy = cv2.findContours(target_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\n",
    "# for i in contours:\n",
    "#     match = cv2.matchShapes(second_largest_contour, i, 1, 0)\n",
    "#     if match < 0.2: \n",
    "#         closest_contour = i\n",
    "#     else: \n",
    "#         closest_contour = [0]\n",
    "        \n",
    "# closest_contour_img = cv2.drawContours(img_copy, [closest_contour], -1, (255,0,0), 2)\n",
    "# cv2.imshow('image', closest_contour_img)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner Detection\n",
    "* Harris Corners are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "img_copy = img.copy()\n",
    "gray_img_copy = gray_img.copy()\n",
    "\n",
    "harris_corner_info = cv2.cornerHarris(gray_img,3,3,0.05)\n",
    "\n",
    "# Dilate the corner points to enlage them\n",
    "kernel_size_5X5 = np.ones((5,5))/25\n",
    "harris_corner_info = cv2.dilate(harris_corner_info,kernel_size_5X5)\n",
    "\n",
    "img_copy[ harris_corner_info > 0.025 * harris_corner_info.max() ] = [255,0,0]\n",
    "cv2.imshow('image', img_copy)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Eye Detection using HAAR Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "img_copy = img.copy()\n",
    "gray_img_copy = gray_img.copy()\n",
    "\n",
    "# Load cascade classifier\n",
    "classify_face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "classify_eye = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "def detect_face_and_eye(img_copy):\n",
    "    \n",
    "    # Region of interest of detected face\n",
    "    face_roi = classify_face.detectMultiScale(gray_img_copy, 1.3, 5)\n",
    "\n",
    "    if face_roi is False:\n",
    "        print('No face detected')\n",
    "\n",
    "    for fx,fy,fw,fh in face_roi:\n",
    "        face_detected = cv2.rectangle(img_copy, (fx,fy), (fx+fw,fy+fh), (0,0,255), 2)\n",
    "\n",
    "        gray_img_crop = gray_img_copy[fy:fy+fh, fx:fx+fw]\n",
    "        img_crop = img_copy[fy:fy+fh ,fx:fx+fw]\n",
    "\n",
    "        eye_roi = classify_eye.detectMultiScale(gray_img_crop)\n",
    "        for ex,ey,ew,eh in eye_roi:\n",
    "            eye_detected = cv2.rectangle(img_crop,(ex,ey), (ex+ew,ey+eh), (0,0,255), 2)\n",
    "            \n",
    "        img_crop = cv2.flip(img_crop,1)\n",
    "        return img_crop\n",
    "\n",
    "detect_face_and_eye(img_copy)\n",
    "\n",
    "cv2.imshow('image', face_detected)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imshow('image', eye_detected)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Face and Eye Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imshow('video', detect_face_and_eye(frame))\n",
    "    \n",
    "    # Press q to Quit window\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Analysis using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify_pedestrian = cv2.CascadeClassifier('haarcascade_fullbody.xml')\n",
    "# cam = cv2.VideoCapture('video1.mp4')\n",
    "\n",
    "# # Loop through video\n",
    "# while cam.isOpened():\n",
    "#     ret, frame = cam.read()\n",
    "#     frame = cv2.resize(frame, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "#     gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     pedestrian_roi = classify_pedestrian.detectMultiScale(gray_img, 1.3, 5)\n",
    "    \n",
    "#     for x,y,w,h in pedestrian_detected:\n",
    "#         pedestrian_detected = cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "#         cv2.imshow('video', pedestrian_detected)\n",
    "    \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cam.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "* A tensor is a N dimensional array of data\n",
    "* Rank 0 = scalar, Rank 1 = vector, Rank 2 = matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version is:  2.4.1\n",
      "Number of GPUs available:  0\n",
      "List of physical devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "Built with CUDA:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version is: ', tf.__version__)\n",
    "print('Number of GPUs available: ',len(tf.config.experimental.list_physical_devices('GPU')) )\n",
    "print('List of physical devices: ', tf.config.experimental.list_physical_devices())\n",
    "print('Built with CUDA: ', tf.test.is_built_with_cuda())\n",
    "\n",
    "# using GPU on code:\n",
    "# with tf.device('/GPU:0'):\n",
    "    #insert model building code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN (Artificial Neural Network)\n",
    "\n",
    "* Activation function:\n",
    "    * sigmoid has value between 0 and 1, 0 = neuron not firing, 1 = neuron firing \n",
    "    * tanh has value between -1 and 1\n",
    "    * relu is used for hidden layers and sigmoid is used at output layer\n",
    "    * relu helps make the model non-linear\n",
    "    * drawback is vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Handling highly imbalanced data\n",
    "    # Method: oversampling minority class using SMOTE (Synthetic Minority Oversampling Technique)\n",
    "        # smote = SMOTE(sampling_strategy='minority')\n",
    "        # x_sm, y_sm = smote.fit_sample(x, y)\n",
    "        # from sklearn.model_selection import train_test_split\n",
    "        # x_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)\n",
    "\n",
    "# import data\n",
    "(x_train, y_train) , (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normalize data for better accuracy of neural network\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# visualize data\n",
    "plt.matshow(x_train[0])\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5148 - accuracy: 0.8470\n",
      "313/313 [==============================] - 0s 970us/step - loss: 0.1487 - accuracy: 0.9568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14873278141021729, 0.9567999839782715]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a neural network \n",
    "# flatten input into 1D array\n",
    "# 28X28 node layer flattened and passed on to a 100 node layer ending at a 10 node layer (having digits 0 to 9 as probabilities) \n",
    "# Dropout regularization helps stop overfitting of model by dropping neurons randomly \n",
    "# compile, fit and evaluate\n",
    "\n",
    "model_sequential = keras.Sequential([\n",
    "    keras.layers.Input(shape=(28,28)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_sequential.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\", histogram_freq=1)\n",
    "\n",
    "model_sequential.fit(x_train, y_train, epochs=1, callbacks=[tb_callback])\n",
    "\n",
    "model_sequential.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3678 - accuracy: 0.8919\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10774016380310059, 0.9670000076293945]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = keras.layers.Input(shape=(28,28))\n",
    "x = keras.layers.Flatten()(inputs)\n",
    "x = keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "outputs = keras.layers.Dense(10, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_functional = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_functional.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\", histogram_freq=1)\n",
    "\n",
    "model_functional.fit(x_train, y_train, epochs=1, callbacks=[tb_callback])\n",
    "\n",
    "model_functional.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensorboard\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# store predictions\n",
    "# digits 0 to 9 are as probabilities, argmax is used to give index of max probability of an array\n",
    "y_predict = model_functional.predict(x_test)\n",
    "\n",
    "y_predict_labels = [np.argmax(i) for i in y_predict]\n",
    "\n",
    "plt.matshow(x_test[0])\n",
    "y_predict_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
       "array([[ 966,    0,    0,    1,    2,    3,    2,    1,    3,    2],\n",
       "       [   0, 1118,    3,    1,    0,    1,    5,    1,    6,    0],\n",
       "       [   6,    0,  986,    8,    7,    1,    2,    8,   13,    1],\n",
       "       [   0,    0,    6,  978,    0,    4,    0,    5,    7,   10],\n",
       "       [   0,    0,    2,    0,  974,    0,    2,    0,    0,    4],\n",
       "       [   3,    0,    0,    8,    3,  858,    7,    1,    5,    7],\n",
       "       [   6,    3,    1,    1,    9,    7,  929,    0,    2,    0],\n",
       "       [   1,    4,   10,    1,    5,    0,    0,  975,    3,   29],\n",
       "       [   4,    0,    1,    6,   10,    5,    6,    5,  930,    7],\n",
       "       [   3,    4,    0,    6,   28,    4,    0,    3,    5,  956]])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusuion matrix\n",
    "\n",
    "cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predict_labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.98      0.96      0.97      1032\n",
      "           3       0.97      0.97      0.97      1010\n",
      "           4       0.94      0.99      0.96       982\n",
      "           5       0.97      0.96      0.97       892\n",
      "           6       0.97      0.97      0.97       958\n",
      "           7       0.98      0.95      0.96      1028\n",
      "           8       0.95      0.95      0.95       974\n",
      "           9       0.94      0.95      0.94      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# f1 score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "print(classification_report(y_test, y_predict_labels))\n",
    "\n",
    "# Layers of the model\n",
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image prepocessing using cv2\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'pic1.jpg'\n",
    "resize_shape = (224, 224)\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Resize\n",
    "resized_img = cv2.resize(img, resize_shape)\n",
    "cv2.imshow('image',resized_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Grayscale\n",
    "gray_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Normalize\n",
    "resized_img = np.array(resized_img)/255\n",
    "\n",
    "# Add an index \n",
    "# img.shape is (1,224,224,3) 1 is the index for better identification of the image\n",
    "resized_img = resized_img[np.newaxis]\n",
    "\n",
    "cv2.imshow('image',np.squeeze(resized_img))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Extraction: Convolution + ReLU --> Pooling layer --> Flatten to 1D Array\n",
    "* CNN Process: Feature extraction --> Classification(ANN)\n",
    "* Convolution is multiplication of dataset matrix and filter matrix\n",
    "* relu helps make the model non-linear\n",
    "* Pooling layer is used to reduce size of matrix \n",
    "    * 2X2 filter finds maximum out of the 4 elements with stride = 1 \n",
    "* Used mainly for image classification \n",
    "* CNN doesn't take care of rotation and scale hence data augmentation is used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import data\n",
    "(x_train, y_train) , (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# normalize data for better accuracy of neural network\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# flatten y_train and y_test \n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# visualize data\n",
    "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "plt.imshow(x_train[0])\n",
    "classes[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation: Helps alter image dataset by flip, rotation, zoom to improve model accuracy\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.Input(shape=(img_height, img_width, 3)),\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    keras.layers.experimental.preprocessing.RandomZoom(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 104s 58ms/step - loss: 1.7760 - accuracy: 0.3777\n",
      "313/313 [==============================] - 5s 10ms/step - loss: 1.3557 - accuracy: 0.5290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3557493686676025, 0.5289999842643738]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution+relu --> MaxPooling --> Convolution+relu --> MaxPooling --> Flatten to 1D array --> Classification\n",
    "# Convolution+relu: \n",
    "    # 32 filters of size 3X3 matrix convolute with input matrix of size 32 X 32 X 3 (3 is for BGR Channel)\n",
    "    # Give inside Conv2D:\n",
    "    # padding = 'same'\n",
    "        # creates a boundary of 1 extra row and 1 extra column around the image (optional)\n",
    "    # strides = (1,1)\n",
    "        # It is how many cells does the filter move while convoluting around the input matrix (optional)\n",
    "# Batch normalization: Automatically standardizes the inputs,  accelerates the training process\n",
    "# MaxPooling: 2X2 matrix with stride = 1 finds max out of 4 elements and reduces matrix size \n",
    "# Dropout regularization helps stop overfitting of model by dropping neurons randomly \n",
    "# softmax activation function is popular in CNN for giving a probabilistic array output\n",
    "\n",
    "# compile, fit, evaluate\n",
    "\n",
    "cnn = keras.Sequential([\n",
    "    \n",
    "    data_augmentation,\n",
    "    keras.layers.Input(shape= (32,32,3)),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "cnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# store predictions\n",
    "# image classes are stored as probabilities, argmax is used to give index of max probability of an array\n",
    "y_predict = cnn.predict(x_test)\n",
    "\n",
    "y_predict_labels = [np.argmax(i) for i in y_predict]\n",
    "\n",
    "plt.imshow(x_test[0])\n",
    "classes[y_predict_labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load or Delete Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('tfmodel')\n",
    "# model = load_model('tfmodel')\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "* Inputs to a model are called feature vectors (binary)\n",
    "* Outputs of a model are binary corresponding to classes which contain the classification or regressor value\n",
    "* 3 Methods of implementation: \n",
    "    * Sequential API Pretrained Model from a tensorflow hub link    \n",
    "    * Functional API Pretrained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential API Pretrained Model from a tensorflow hub link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image prepocessing using cv2\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'pic1.jpg'\n",
    "resize_shape = (224, 224)\n",
    "img = cv2.imread(img_path)\n",
    "resized_img = cv2.resize(img, resize_shape)\n",
    "resized_img = np.array(resized_img)/255\n",
    "\n",
    "# Add an index \n",
    "# img.shape is (1,224,224,3) 1 is the index for better identification of the image\n",
    "resized_img = resized_img[np.newaxis]\n",
    "\n",
    "cv2.imshow('image',np.squeeze(resized_img))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Pre-trained model from Tensorflow Hub\n",
    "classifier = keras.Sequential([\n",
    "    keras.layers.Input(shape=resize_shape+(3,)),\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict image using pretrained model\n",
    "result = classifier.predict(resized_img)\n",
    "predicted_label_index = np.argmax(result)\n",
    "keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "\n",
    "image_labels = []\n",
    "with open(\"ImageNetLabels.txt\", \"r\") as f:\n",
    "    image_labels = f.read().splitlines()\n",
    "\n",
    "output = image_labels[predicted_label_index]\n",
    "\n",
    "x_train = resized_img\n",
    "# y_train = tf.constant([0])\n",
    "y = { \n",
    "    'faces': 0,\n",
    "    'eyes':  1\n",
    "    }\n",
    "y_list = []\n",
    "y_list.append(y['faces'])\n",
    "y_train = np.array(y_list)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_7 (Sequential)    (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train your own model\n",
    "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "\n",
    "pretrained_model_without_top_layer = keras.Sequential([\n",
    "    keras.layers.Input(shape=resize_shape+(3,)),\n",
    "    hub.KerasLayer(feature_extractor_model, trainable=False)\n",
    "\n",
    "])\n",
    "\n",
    "model_tf_hub = keras.Sequential([\n",
    "  pretrained_model_without_top_layer,\n",
    "  keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model_tf_hub.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_tf_hub.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model_tf_hub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict input using newly trained model\n",
    "newly_trained_result = model_tf_hub.predict(resized_img)\n",
    "\n",
    "predicted_label_index = np.argmax(newly_trained_result)\n",
    "predicted_label_index\n",
    "# 0 is for 'faces' in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional API Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# # ================================================ #\n",
    "# #                  Pretrained-Model                #\n",
    "# # ================================================ #\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "# x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# model = keras.models.load_model(\"pretrained\")\n",
    "\n",
    "# # Freeze all model layer weights\n",
    "# model.trainable = False\n",
    "\n",
    "# # Can also set trainable for specific layers\n",
    "# for layer in model.layers:\n",
    "#     # assert should be true because of one-liner above\n",
    "#     assert layer.trainable == False\n",
    "#     layer.trainable = False\n",
    "\n",
    "# print(model.summary())  # for finding base input and output\n",
    "# base_inputs = model.layers[0].input\n",
    "# base_output = model.layers[-2].output\n",
    "# output = layers.Dense(10)(base_output)\n",
    "# new_model = keras.Model(base_inputs, output)\n",
    "\n",
    "# # This model is actually identical to model we\n",
    "# # loaded (this is just for demonstration and\n",
    "# # and not something you would do in practice).\n",
    "# print(new_model.summary())\n",
    "\n",
    "# # As usual we do compile and fit, this time on new_model\n",
    "# new_model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(),\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "\n",
    "# new_model.fit(x_train, y_train, batch_size=32, epochs=3, verbose=2)\n",
    "\n",
    "# # =================================================== #\n",
    "# #                Pretrained Keras Model               #\n",
    "# # =================================================== #\n",
    "\n",
    "# # Random data for demonstration (3 examples w. 3 classes)\n",
    "# x = tf.random.normal(shape=(3, 299, 299, 3))\n",
    "# y = tf.constant([0, 1, 2])\n",
    "\n",
    "# model = keras.applications.InceptionV3(include_top=True)\n",
    "# print(model.summary())\n",
    "\n",
    "# # for input you can also do model.input,\n",
    "# # then for base_outputs you can obviously\n",
    "# # choose other than simply removing the last one :)\n",
    "# base_inputs = model.layers[0].input\n",
    "# base_outputs = model.layers[-2].output\n",
    "# classifier = layers.Dense(3)(base_outputs)\n",
    "# new_model = keras.Model(inputs=base_inputs, outputs=classifier)\n",
    "# new_model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(),\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "\n",
    "# print(new_model.summary())\n",
    "# new_model.fit(x, y, epochs=15, verbose=2)\n",
    "\n",
    "# # ================================================= #\n",
    "# #                Pretrained Hub Model               #\n",
    "# # ================================================= #\n",
    "\n",
    "# # Random data for demonstration (3 examples w. 3 classes)\n",
    "# x = tf.random.normal(shape=(3, 299, 299, 3))\n",
    "# y = tf.constant([0, 1, 2])\n",
    "\n",
    "# url = \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4\"\n",
    "\n",
    "# base_model = hub.KerasLayer(url, input_shape=(299, 299, 3))\n",
    "# model = keras.Sequential(\n",
    "#     [\n",
    "#         base_model,\n",
    "#         layers.Dense(128, activation=\"relu\"),\n",
    "#         layers.Dense(64, activation=\"relu\"),\n",
    "#         layers.Dense(10),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(),\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "\n",
    "# model.fit(x, y, batch_size=32, epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
